{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Household Power Consumption: prediction of electric usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We develop a model that predicts household electric power consumption based on previous usage. The model needs to infer the next twenty four observations from the past twenty four. The baseline model given to beat throws a validation MAE of approximately 0.055. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the dataset and explore some of its statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_table('household_power_consumption.csv', sep=',')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is quite clean so we procede to the other steps in preparation for training a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the dataset for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ = data.values[:,1:]\n",
    "\n",
    "data_min = np.min(data_, axis=0)\n",
    "data_max = np.max(data_, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_time = 69120\n",
    "\n",
    "t_train = np.array(range(0,split_time+1))\n",
    "x_train = (data_[:split_time,:]-data_min)/data_max\n",
    "\n",
    "t_test = np.array(range(split_time+1,data.shape[0]+1))\n",
    "x_test = (data_[split_time:,:]-data_min)/data_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_past = 24  \n",
    "n_future = 24 \n",
    "window_size = n_past + n_future\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "for i in range(x_train.shape[0]-window_size):\n",
    "    X_train.append(x_train[i:i+n_past,:])\n",
    "    Y_train.append(x_train[i+n_past:i+window_size,:])\n",
    "X_train = np.array(X_train, dtype='float32')\n",
    "Y_train = np.array(Y_train, dtype='float32')\n",
    "\n",
    "X_test = []\n",
    "Y_test = []\n",
    "for i in range(x_test.shape[0]-window_size):\n",
    "    X_test.append(x_test[i:i+n_past,:])\n",
    "    Y_test.append(x_test[i+n_past:i+window_size,:])\n",
    "X_test = np.array(X_test, dtype='float32')\n",
    "Y_test = np.array(Y_test, dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we train our first model, a Deep Neural Network with LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_1():\n",
    "    f1 = tf.keras.layers.LSTM(units=128,\n",
    "                              activation='tanh',\n",
    "                              recurrent_activation='sigmoid',\n",
    "                              kernel_initializer='glorot_uniform',\n",
    "                              bias_initializer='zeros',\n",
    "                              recurrent_initializer='zeros',\n",
    "                              return_sequences=True,\n",
    "                              return_state = False)\n",
    "    f2 = tf.keras.layers.Dropout(rate=0.5)\n",
    "    f3 = tf.keras.layers.Dense(units=Y_train.shape[1:][1],\n",
    "                              activation='linear',\n",
    "                              kernel_initializer='glorot_uniform',\n",
    "                              bias_initializer='zeros')\n",
    "    x = tf.keras.Input(shape=X_train.shape[1:])\n",
    "    a1 = f1(x)\n",
    "    a2 = f2(a1)\n",
    "    y = f3(a2)\n",
    "    model = tf.keras.Model(x, y)   \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, \n",
    "                                         beta_1=0.9, \n",
    "                                         beta_2=0.999, \n",
    "                                         epsilon=1e-07)\n",
    "    model.compile(loss='huber_loss', metrics=['mae'], optimizer=optimizer)\n",
    "    model.summary()\n",
    "    callback1 = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', \n",
    "                                                     patience=10,\n",
    "                                                     min_delta=0.001,\n",
    "                                                     factor=0.1, \n",
    "                                                     min_lr=0.0001)\n",
    "    callback2 = tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                                                 patience=20,\n",
    "                                                 min_delta=0.001)\n",
    "    model.fit(X_train, Y_train, epochs=100, batch_size=64, callbacks=[callback1, callback2])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = Model_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.evaluate(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first model is performing worse than the baseline model so we discard it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we train our second model, a Deep Neural Network with Bidirectional LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_2():\n",
    "    f1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=128,\n",
    "                                                            activation='tanh',\n",
    "                                                            recurrent_activation='sigmoid',\n",
    "                                                            kernel_initializer='glorot_uniform',\n",
    "                                                            bias_initializer='zeros',\n",
    "                                                            recurrent_initializer='zeros',\n",
    "                                                            return_sequences=True,\n",
    "                                                            return_state = False), \n",
    "                                       merge_mode='concat')\n",
    "    f2 = tf.keras.layers.Dropout(rate=0.5)\n",
    "    f3 = tf.keras.layers.Dense(units=128,\n",
    "                               activation='relu',\n",
    "                               kernel_initializer='glorot_uniform',\n",
    "                               bias_initializer='zeros')\n",
    "    f4 = tf.keras.layers.Dropout(rate=0.5)\n",
    "    f5 = tf.keras.layers.Dense(units=128,\n",
    "                               activation='relu',\n",
    "                               kernel_initializer='glorot_uniform',\n",
    "                               bias_initializer='zeros')\n",
    "    f6 = tf.keras.layers.Dropout(rate=0.5)\n",
    "    f7 = tf.keras.layers.Dense(units=Y_train.shape[1:][1],\n",
    "                              activation='linear',\n",
    "                              kernel_initializer='glorot_uniform',\n",
    "                              bias_initializer='zeros')\n",
    "    x = tf.keras.Input(shape=X_train.shape[1:])\n",
    "    a1 = f1(x)\n",
    "    a2 = f2(a1)\n",
    "    a3 = f3(a2)\n",
    "    a4 = f4(a3)\n",
    "    a5 = f5(a4)\n",
    "    a6 = f6(a5)\n",
    "    y = f7(a6)\n",
    "    model = tf.keras.Model(x, y)   \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, \n",
    "                                         beta_1=0.9, \n",
    "                                         beta_2=0.999, \n",
    "                                         epsilon=1e-07)\n",
    "    model.compile(loss='huber_loss', metrics=['mae'], optimizer=optimizer)\n",
    "    model.summary()\n",
    "    callback1 = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', \n",
    "                                                     patience=10,\n",
    "                                                     min_delta=0.001,\n",
    "                                                     factor=0.1, \n",
    "                                                     min_lr=0.0001)\n",
    "    callback2 = tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                                                 patience=20,\n",
    "                                                 min_delta=0.001)\n",
    "    model.fit(X_train, Y_train, epochs=100, batch_size=64, callbacks=[callback1, callback2])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Model_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.evaluate(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This second model is also performing worse than the baseline model so we discard it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we train our third and final model, a Deep Neural Network with Conv1D and Bidirectional LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_3():\n",
    "    f1 = tf.keras.layers.Conv1D(filters=256, \n",
    "                                kernel_size=5, \n",
    "                                strides=1,\n",
    "                                padding='causal',\n",
    "                                activation='relu',\n",
    "                                kernel_initializer='glorot_uniform',\n",
    "                                bias_initializer='zeros')\n",
    "    f2 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=128,\n",
    "                                                            activation='tanh',\n",
    "                                                            recurrent_activation='sigmoid',\n",
    "                                                            kernel_initializer='glorot_uniform',\n",
    "                                                            bias_initializer='zeros',\n",
    "                                                            recurrent_initializer='zeros',\n",
    "                                                            return_sequences=True,\n",
    "                                                            return_state = False), \n",
    "                                       merge_mode='concat')\n",
    "    f3 = tf.keras.layers.Dropout(rate=0.5)\n",
    "    f4 = tf.keras.layers.Dense(units=128,\n",
    "                               activation='relu',\n",
    "                               kernel_initializer='glorot_uniform',\n",
    "                               bias_initializer='zeros')\n",
    "    f5 = tf.keras.layers.Dropout(rate=0.5)\n",
    "    f6 = tf.keras.layers.Dense(units=128,\n",
    "                               activation='relu',\n",
    "                               kernel_initializer='glorot_uniform',\n",
    "                               bias_initializer='zeros')\n",
    "    f7 = tf.keras.layers.Dropout(rate=0.5)\n",
    "    f8 = tf.keras.layers.Dense(units=Y_train.shape[1:][1],\n",
    "                                activation='linear',\n",
    "                                kernel_initializer='glorot_uniform',\n",
    "                                bias_initializer='zeros')\n",
    "    x = tf.keras.Input(shape=X_train.shape[1:])\n",
    "    a1 = f1(x)\n",
    "    a2 = f2(a1)\n",
    "    a3 = f3(a2)\n",
    "    a4 = f4(a3)\n",
    "    a5 = f5(a4)\n",
    "    a6 = f6(a5)\n",
    "    a7 = f7(a6)\n",
    "    y = f8(a7)\n",
    "    model = tf.keras.Model(x, y)   \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, \n",
    "                                         beta_1=0.9, \n",
    "                                         beta_2=0.999, \n",
    "                                         epsilon=1e-07)\n",
    "    model.compile(loss='huber_loss', metrics=['mae'], optimizer=optimizer)\n",
    "    model.summary()\n",
    "    callback1 = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', \n",
    "                                                     patience=5,\n",
    "                                                     min_delta=0.001,\n",
    "                                                     factor=0.1, \n",
    "                                                     min_lr=0.0001)\n",
    "    callback2 = tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                                                 patience=20,\n",
    "                                                 min_delta=0.001)\n",
    "    model.fit(X_train, Y_train, epochs=100, batch_size=64, callbacks=[callback1, callback2])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_3 = Model_3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.evaluate(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As we can see, this last model beats the provided baseline model MAE, so it could be considered a candidate solution for the original problem presented. Additional exploration of hyperparameters would be necessary to make a final conclusion. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
